# üß† Brand Guidelines LLM Integration Setup Guide

## üéØ √úbersicht

Dieses System integriert die neuesten OpenAI-Modelle (GPT-5, text-embedding-3-large) f√ºr semantische Suche und automatische Compliance-Pr√ºfung von Brand Guidelines.

## üöÄ Schnellstart

### 1. OpenAI API Key einrichten

```bash
# API Key setzen
export OPENAI_API_KEY="your-openai-api-key-here"

# Oder in .env Datei (falls erstellt)
echo "OPENAI_API_KEY=your-openai-api-key-here" >> .env
```

### 2. Services starten

```bash
# Alle Services starten
docker-compose up -d

# Status √ºberpr√ºfen
docker-compose ps
```

### 3. Datenbank-Schema aktualisieren

```bash
# Neues Schema mit 3072-dimensionalen Embeddings
docker-compose exec brandchecker-postgres psql -U brandchecker_user -d brandchecker -f /docker-entrypoint-initdb.d/02_brand_guidelines_schema.sql
```

### 4. Brand Guidelines importieren

```bash
# JSON-Daten importieren
docker-compose exec brandchecker-python python /app/brand_guidelines_importer.py
```

### 5. Embeddings generieren

```bash
# OpenAI Embeddings erstellen (kann einige Minuten dauern)
docker-compose exec llm-api-service python /app/embedding_service.py
```

### 6. Indexes optimieren

```bash
# Vector-Indexes f√ºr beste Performance optimieren
docker-compose exec brandchecker-python python /app/optimize_indexes.py
```

### 7. System testen

```bash
# Vollst√§ndigen Test durchf√ºhren
python test_llm_system.py
```

## üìä Modelle & Konfiguration

### Aktuelle Modelle (neueste verf√ºgbare)

| Komponente | Prim√§res Modell | Fallback-Modell | Dimensionen |
|------------|----------------|-----------------|-------------|
| **Embeddings** | `text-embedding-3-large` | `text-embedding-3-small` | 3072 |
| **LLM** | `gpt-5` | `gpt-4o` | - |

### Warum diese Modelle?

- **text-embedding-3-large**: Beste Performance f√ºr semantische Suche, bis zu 3072 Dimensionen
- **gpt-5**: Neuestes und leistungsst√§rkstes Modell (August 2025), 400k Token Kontext
- **Fallback-System**: Automatische Nutzung √§lterer Modelle falls neue nicht verf√ºgbar

## üîß API Endpoints

### LLM API Service (Port 8001)

| Endpoint | Methode | Beschreibung |
|----------|---------|--------------|
| `/health` | GET | Service-Status |
| `/api/brands` | GET | Alle verf√ºgbaren Brands |
| `/api/search` | POST | Semantische Suche |
| `/api/ask` | POST | LLM-Fragen & Antworten |
| `/api/compliance/check` | POST | Compliance-Pr√ºfung |
| `/api/embeddings/status` | GET | Embedding-Status |
| `/api/embeddings/generate` | POST | Embeddings generieren |

### Beispiel-Requests

#### Semantische Suche
```bash
curl -X POST http://localhost:8001/api/search \
  -H "Content-Type: application/json" \
  -d '{
    "query": "Welche Farben sind f√ºr Bosch Corporate erlaubt?",
    "brand_id": "8a9239a4-18f6-4aca-b9fe-08426bd01825",
    "limit": 5
  }'
```

#### LLM-Frage
```bash
curl -X POST http://localhost:8001/api/ask \
  -H "Content-Type: application/json" \
  -d '{
    "question": "Welche Schriftarten darf ich f√ºr Bosch verwenden?",
    "brand_id": "8a9239a4-18f6-4aca-b9fe-08426bd01825"
  }'
```

#### Compliance-Pr√ºfung
```bash
curl -X POST http://localhost:8001/api/compliance/check \
  -H "Content-Type: application/json" \
  -d '{
    "content": "Ich verwende die Farbe #FF0000 f√ºr mein Bosch-Logo.",
    "brand_id": "8a9239a4-18f6-4aca-b9fe-08426bd01825",
    "check_type": "colors"
  }'
```

## üóÑÔ∏è Datenbankstruktur

### Optimierte Vector-Indexes

- **ivfflat Index**: F√ºr schnelle Cosine-Similarity-Suche
- **Optimierte Parameter**: Automatische Berechnung basierend auf Datenmenge
- **Composite Indexes**: F√ºr brand-spezifische Abfragen
- **Materialized Views**: F√ºr Performance-Optimierung

### Embedding-Tabellen

```sql
-- Brand Knowledge Chunks (3072 Dimensionen)
brand_knowledge_chunks (
    id UUID,
    brand_id UUID,
    chunk_type VARCHAR(50),
    content TEXT,
    embedding vector(3072),
    embedding_model VARCHAR(50),
    metadata JSONB
)

-- Knowledge Queries
knowledge_queries (
    id UUID,
    query_text TEXT,
    query_embedding vector(3072),
    response_text TEXT
)
```

## üîç Use Cases

### 1. Semantische Suche
- **Frage**: "Welche Farben sind f√ºr Bosch erlaubt?"
- **System**: Findet relevante Brand-Guidelines automatisch
- **Antwort**: Konkrete Farbangaben mit HEX-Codes und Verwendungsregeln

### 2. Compliance-Checking
- **Input**: PDF-Dokument oder Text-Inhalt
- **System**: Pr√ºft automatisch gegen Brand-Standards
- **Output**: Compliance-Score und spezifische Empfehlungen

### 3. Asset-Empfehlungen
- **Kontext**: "Ich brauche ein Logo f√ºr Social Media"
- **System**: Empfiehlt passende Assets basierend auf Guidelines
- **Output**: Spezifische Asset-URLs und Verwendungsregeln

### 4. Brand-Guideline Q&A
- **Frage**: "Darf ich das Bosch-Logo in schwarz-wei√ü verwenden?"
- **System**: Durchsucht Guidelines und gibt pr√§zise Antwort
- **Output**: Ja/Nein mit Begr√ºndung und Verweisen

## ‚ö° Performance-Optimierungen

### Vector-Index-Optimierung
```bash
# Indexes mit optimalen Parametern erstellen
python optimize_indexes.py
```

### Automatische Fallbacks
- **GPT-5 nicht verf√ºgbar** ‚Üí Automatischer Wechsel zu GPT-4o
- **text-embedding-3-large nicht verf√ºgbar** ‚Üí Fallback zu text-embedding-3-small
- **Rate-Limiting** ‚Üí Automatische Verz√∂gerungen und Batch-Verarbeitung

### Caching-Strategien
- **Embedding-Cache**: Bereits generierte Embeddings werden wiederverwendet
- **Materialized Views**: H√§ufige Abfragen werden vorberechnet
- **Connection Pooling**: Optimierte Datenbankverbindungen

## üîß Troubleshooting

### H√§ufige Probleme

#### 1. OpenAI API Key nicht gesetzt
```bash
# Fehler: "OPENAI_API_KEY environment variable is required"
export OPENAI_API_KEY="your-key-here"
docker-compose restart llm-api-service
```

#### 2. GPT-5 nicht verf√ºgbar
```bash
# System nutzt automatisch GPT-4o als Fallback
# Logs zeigen: "Model gpt-5 not available, trying fallback..."
```

#### 3. Embeddings nicht generiert
```bash
# Embeddings manuell generieren
docker-compose exec llm-api-service python /app/embedding_service.py
```

#### 4. Langsame Performance
```bash
# Indexes optimieren
docker-compose exec brandchecker-python python /app/optimize_indexes.py
```

### Logs √ºberpr√ºfen

```bash
# LLM API Service Logs
docker-compose logs llm-api-service

# Embedding Service Logs
docker-compose logs brandchecker-python

# PostgreSQL Logs
docker-compose logs brandchecker-postgres
```

## üìà Monitoring

### Embedding-Status √ºberpr√ºfen
```bash
curl http://localhost:8001/api/embeddings/status
```

### Performance-Metriken
- **Embedding-Coverage**: Prozentsatz der indexierten Inhalte
- **Search-Latency**: Zeit f√ºr semantische Suche
- **LLM-Response-Time**: Zeit f√ºr AI-Antworten
- **Index-Performance**: Vector-Index-Effizienz

## üöÄ N√§chste Schritte

1. **n8n Integration**: LLM-API in n8n-Workflows einbinden
2. **PDF-Integration**: Automatische Compliance-Pr√ºfung von PDF-Dokumenten
3. **Real-time Updates**: Automatische Embedding-Updates bei neuen Guidelines
4. **Multi-Brand Support**: Erweiterung f√ºr mehrere Brand-Guidelines
5. **Advanced Analytics**: Detaillierte Compliance-Reports und Trends

## üí° Tipps

- **Batch-Processing**: Embeddings in kleinen Batches generieren f√ºr bessere Stabilit√§t
- **Rate-Limiting**: OpenAI API-Limits beachten (60 Requests/Minute f√ºr Embeddings)
- **Backup**: Regelm√§√üige Backups der Embedding-Daten
- **Updates**: System regelm√§√üig auf neue Modelle aktualisieren

---

**üéâ Ihr Brand Guidelines LLM-System ist jetzt bereit f√ºr den produktiven Einsatz!**
